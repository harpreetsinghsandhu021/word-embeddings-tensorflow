{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5d945ef-ff53-4c3a-b60a-f68d988adcc6",
   "metadata": {},
   "source": [
    "# Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "19672b5d-8dc2-46f3-b4bf-d8da7d266986",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow-macos\n",
      "  Using cached tensorflow_macos-2.16.2-cp39-cp39-macosx_12_0_arm64.whl.metadata (3.3 kB)\n",
      "Collecting tensorflow-metal\n",
      "  Using cached tensorflow_metal-1.1.0-cp39-cp39-macosx_12_0_arm64.whl.metadata (1.2 kB)\n",
      "Collecting tensorflow==2.16.2 (from tensorflow-macos)\n",
      "  Using cached tensorflow-2.16.2-cp39-cp39-macosx_12_0_arm64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/anaconda3/envs/word-embeddings/lib/python3.9/site-packages (from tensorflow==2.16.2->tensorflow-macos) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/anaconda3/envs/word-embeddings/lib/python3.9/site-packages (from tensorflow==2.16.2->tensorflow-macos) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /opt/anaconda3/envs/word-embeddings/lib/python3.9/site-packages (from tensorflow==2.16.2->tensorflow-macos) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/anaconda3/envs/word-embeddings/lib/python3.9/site-packages (from tensorflow==2.16.2->tensorflow-macos) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/anaconda3/envs/word-embeddings/lib/python3.9/site-packages (from tensorflow==2.16.2->tensorflow-macos) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in /opt/anaconda3/envs/word-embeddings/lib/python3.9/site-packages (from tensorflow==2.16.2->tensorflow-macos) (3.12.1)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/anaconda3/envs/word-embeddings/lib/python3.9/site-packages (from tensorflow==2.16.2->tensorflow-macos) (18.1.1)\n",
      "Collecting ml-dtypes~=0.3.1 (from tensorflow==2.16.2->tensorflow-macos)\n",
      "  Using cached ml_dtypes-0.3.2-cp39-cp39-macosx_10_9_universal2.whl.metadata (20 kB)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/anaconda3/envs/word-embeddings/lib/python3.9/site-packages (from tensorflow==2.16.2->tensorflow-macos) (3.4.0)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/envs/word-embeddings/lib/python3.9/site-packages (from tensorflow==2.16.2->tensorflow-macos) (24.2)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow==2.16.2->tensorflow-macos)\n",
      "  Using cached protobuf-4.25.5-cp37-abi3-macosx_10_9_universal2.whl.metadata (541 bytes)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/anaconda3/envs/word-embeddings/lib/python3.9/site-packages (from tensorflow==2.16.2->tensorflow-macos) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/envs/word-embeddings/lib/python3.9/site-packages (from tensorflow==2.16.2->tensorflow-macos) (75.6.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/anaconda3/envs/word-embeddings/lib/python3.9/site-packages (from tensorflow==2.16.2->tensorflow-macos) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/anaconda3/envs/word-embeddings/lib/python3.9/site-packages (from tensorflow==2.16.2->tensorflow-macos) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/anaconda3/envs/word-embeddings/lib/python3.9/site-packages (from tensorflow==2.16.2->tensorflow-macos) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/anaconda3/envs/word-embeddings/lib/python3.9/site-packages (from tensorflow==2.16.2->tensorflow-macos) (1.17.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/anaconda3/envs/word-embeddings/lib/python3.9/site-packages (from tensorflow==2.16.2->tensorflow-macos) (1.68.1)\n",
      "Collecting tensorboard<2.17,>=2.16 (from tensorflow==2.16.2->tensorflow-macos)\n",
      "  Using cached tensorboard-2.16.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: keras>=3.0.0 in /opt/anaconda3/envs/word-embeddings/lib/python3.9/site-packages (from tensorflow==2.16.2->tensorflow-macos) (3.7.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/anaconda3/envs/word-embeddings/lib/python3.9/site-packages (from tensorflow==2.16.2->tensorflow-macos) (0.37.1)\n",
      "Collecting numpy<2.0.0,>=1.23.5 (from tensorflow==2.16.2->tensorflow-macos)\n",
      "  Using cached numpy-1.26.4-cp39-cp39-macosx_11_0_arm64.whl.metadata (61 kB)\n",
      "Requirement already satisfied: wheel~=0.35 in /opt/anaconda3/envs/word-embeddings/lib/python3.9/site-packages (from tensorflow-metal) (0.45.1)\n",
      "Requirement already satisfied: rich in /opt/anaconda3/envs/word-embeddings/lib/python3.9/site-packages (from keras>=3.0.0->tensorflow==2.16.2->tensorflow-macos) (13.9.4)\n",
      "Requirement already satisfied: namex in /opt/anaconda3/envs/word-embeddings/lib/python3.9/site-packages (from keras>=3.0.0->tensorflow==2.16.2->tensorflow-macos) (0.0.8)\n",
      "Requirement already satisfied: optree in /opt/anaconda3/envs/word-embeddings/lib/python3.9/site-packages (from keras>=3.0.0->tensorflow==2.16.2->tensorflow-macos) (0.13.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/anaconda3/envs/word-embeddings/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorflow==2.16.2->tensorflow-macos) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/word-embeddings/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorflow==2.16.2->tensorflow-macos) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/word-embeddings/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorflow==2.16.2->tensorflow-macos) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/word-embeddings/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorflow==2.16.2->tensorflow-macos) (2024.8.30)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/anaconda3/envs/word-embeddings/lib/python3.9/site-packages (from tensorboard<2.17,>=2.16->tensorflow==2.16.2->tensorflow-macos) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/anaconda3/envs/word-embeddings/lib/python3.9/site-packages (from tensorboard<2.17,>=2.16->tensorflow==2.16.2->tensorflow-macos) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/anaconda3/envs/word-embeddings/lib/python3.9/site-packages (from tensorboard<2.17,>=2.16->tensorflow==2.16.2->tensorflow-macos) (3.1.3)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /opt/anaconda3/envs/word-embeddings/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard<2.17,>=2.16->tensorflow==2.16.2->tensorflow-macos) (8.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/anaconda3/envs/word-embeddings/lib/python3.9/site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow==2.16.2->tensorflow-macos) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/anaconda3/envs/word-embeddings/lib/python3.9/site-packages (from rich->keras>=3.0.0->tensorflow==2.16.2->tensorflow-macos) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/envs/word-embeddings/lib/python3.9/site-packages (from rich->keras>=3.0.0->tensorflow==2.16.2->tensorflow-macos) (2.18.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /opt/anaconda3/envs/word-embeddings/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.17,>=2.16->tensorflow==2.16.2->tensorflow-macos) (3.21.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/envs/word-embeddings/lib/python3.9/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow==2.16.2->tensorflow-macos) (0.1.2)\n",
      "Using cached tensorflow_macos-2.16.2-cp39-cp39-macosx_12_0_arm64.whl (2.1 kB)\n",
      "Using cached tensorflow-2.16.2-cp39-cp39-macosx_12_0_arm64.whl (227.0 MB)\n",
      "Using cached tensorflow_metal-1.1.0-cp39-cp39-macosx_12_0_arm64.whl (1.4 MB)\n",
      "Using cached ml_dtypes-0.3.2-cp39-cp39-macosx_10_9_universal2.whl (389 kB)\n",
      "Using cached numpy-1.26.4-cp39-cp39-macosx_11_0_arm64.whl (14.0 MB)\n",
      "Using cached protobuf-4.25.5-cp37-abi3-macosx_10_9_universal2.whl (394 kB)\n",
      "Using cached tensorboard-2.16.2-py3-none-any.whl (5.5 MB)\n",
      "Installing collected packages: tensorflow-metal, protobuf, numpy, ml-dtypes, tensorboard, tensorflow, tensorflow-macos\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 5.29.1\n",
      "    Uninstalling protobuf-5.29.1:\n",
      "      Successfully uninstalled protobuf-5.29.1\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.0.2\n",
      "    Uninstalling numpy-2.0.2:\n",
      "      Successfully uninstalled numpy-2.0.2\n",
      "  Attempting uninstall: ml-dtypes\n",
      "    Found existing installation: ml-dtypes 0.4.1\n",
      "    Uninstalling ml-dtypes-0.4.1:\n",
      "      Successfully uninstalled ml-dtypes-0.4.1\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.18.0\n",
      "    Uninstalling tensorboard-2.18.0:\n",
      "      Successfully uninstalled tensorboard-2.18.0\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 2.18.0\n",
      "    Uninstalling tensorflow-2.18.0:\n",
      "      Successfully uninstalled tensorflow-2.18.0\n",
      "Successfully installed ml-dtypes-0.3.2 numpy-1.26.4 protobuf-4.25.5 tensorboard-2.16.2 tensorflow-2.16.2 tensorflow-macos-2.16.2 tensorflow-metal-1.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tensorflow-macos tensorflow-metal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e29b066-3bc4-424c-9fa8-2214dbd42b07",
   "metadata": {},
   "source": [
    "## Sentiment Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a068a00f-856b-4c3e-93b3-94ac868daa8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import string\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D\n",
    "from tensorflow.keras.layers import TextVectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6d8ab0-e8f6-47c1-921a-9835bcc05a72",
   "metadata": {},
   "source": [
    "### GPU Acceleration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b91b339d-95de-4c87-9ae3-8f59a672889f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices(\"GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dfea2a1a-2373-4e5f-9f13-9320d9665157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(gpus)\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10baeb6b-974c-4b9d-8dda-ba2776808934",
   "metadata": {},
   "source": [
    "## Download the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c655a75b-8d9c-4c95-b242-abf756b54cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
    "\n",
    "dataset = tf.keras.utils.get_file(\"aclImdb_v1.tar.gz\", url,\n",
    "                                  untar=True, cache_dir='.',\n",
    "                                  cache_subdir='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5679a6e-c1bb-4b96-af52-3eb69ebd9c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = 'aclImdb_v1_extracted/aclImdb/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772916dd-5652-4267-b3d9-09af52377500",
   "metadata": {},
   "source": [
    "Our dataset has pos and neg folders with movie reviews labelled as positive and negative respectively. We will use reviews from pos and neg folders to train a binary classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d40615c6-785a-4fc9-b16a-110758ef984e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['urls_unsup.txt',\n",
       " '.DS_Store',\n",
       " 'neg',\n",
       " 'urls_pos.txt',\n",
       " 'urls_neg.txt',\n",
       " 'pos',\n",
       " 'unsupBow.feat',\n",
       " 'labeledBow.feat']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dir = os.path.join(dataset_dir,'train')\n",
    "os.listdir(train_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0ac075-71c2-44d6-833c-0aef7e9c473e",
   "metadata": {},
   "source": [
    "The `Train` directory has additional folders which should be removed before creating training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9cd0aee0-142b-4ff8-afdf-4a655d7928eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_dir = os.path.join(train_dir,'unsup')\n",
    "shutil.rmtree(remove_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a3df88-8d39-4434-94bb-bc29156bbbc9",
   "metadata": {},
   "source": [
    "Use the train directory to create both train and validation datasets with a split of 20% for validation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "644f6fc4-112c-48de-aba7-64883132f721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25001 files belonging to 2 classes.\n",
      "Using 20001 files for training.\n",
      "Found 25001 files belonging to 2 classes.\n",
      "Using 5000 files for validation.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1024\n",
    "seed = 123\n",
    "\n",
    "train_ds = tf.keras.utils.text_dataset_from_directory(train_dir, batch_size=batch_size, validation_split=0.2, subset='training', seed=seed)\n",
    "val_ds = tf.keras.utils.text_dataset_from_directory(train_dir, batch_size=batch_size, validation_split=0.2, subset='validation', seed=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ebb02c-cf17-43be-9ba7-d00d6f65a6b8",
   "metadata": {},
   "source": [
    "Take a look at a few movie reviews and their labels (1: positive, 0: negative) from the train dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c0ea525-4d66-4b7e-9480-8096767e95e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 b\"Oh My God! Please, for the love of all that is holy, Do Not Watch This Movie! It it 82 minutes of my life I will never get back. Sure, I could have stopped watching half way through. But I thought it might get better. It Didn't. Anyone who actually enjoyed this movie is one seriously sick and twisted individual. No wonder us Australians/New Zealanders have a terrible reputation when it comes to making movies. Everything about this movie is horrible, from the acting to the editing. I don't even normally write reviews on here, but in this case I'll make an exception. I only wish someone had of warned me before I hired this catastrophe\"\n",
      "------------------------\n",
      "1 b\"Halloween is the story of a boy who was misunderstood as a child. He takes out his problems on his older sister, whom he murders at the beginning of the film. This is just the start of things to come from Michael Myers.<br /><br />Donald Pleasance plays the doctor who's been studying Myers for years. He knows that something is different about him, something mysteriously evil. This evil will not be contained, and it cannot be stopped.<br /><br />After an escape from an institution, Myers tracks down his younger sister. If he kills her, there may be an end to the troubles of this misunderstood boy. But he seems to have problems in finishing his sister off as other people get in the way. He manages to take them out while still looking for that one girl he needs.<br /><br />There have been a lot of those horror movies involving teenagers getting hacked to pieces by a masked or gruesome killer. But this one started it all, sort of. If you think about it, most of those horror movies we all remember are the ones that have Freddy Kruger or Jason chasing around half naked girls. Well, if it wasn't for Halloween, those characters wouldn't have haunted our dreams when we were children.<br /><br />Halloween's director, John Carpenter, got a lot out of the horror movies of the '50s and combined everything he knew into one film that scared the hell out of a lot of people back in the late '70s. This films solidified him as a director to watch and also jump started the career of Jamie Lee Curtis, who plays the girl being stalked by the masked killer.<br /><br />This film may seem clich\\xc3\\xa9 today, but back then there wasn't much out there like this. It's been copied from and ripped off of, but Halloween will always remain the quintessential teenage horror movie. It still gives you chills listening to Carpenter's thrilling music while we see another victim get chased by that shadowy Michael Myers.\"\n",
      "------------------------\n",
      "0 b\"Alex D. Linz replaces Macaulay Culkin as the central figure in the third movie in the Home Alone empire. Four industrial spies acquire a missile guidance system computer chip and smuggle it through an airport inside a remote controlled toy car. Because of baggage confusion, grouchy Mrs. Hess (Marian Seldes) gets the car. She gives it to her neighbor, Alex (Linz), just before the spies turn up. The spies rent a house in order to burglarize each house in the neighborhood until they locate the car. Home alone with the chicken pox, Alex calls 911 each time he spots a theft in progress, but the spies always manage to elude the police while Alex is accused of making prank calls. The spies finally turn their attentions toward Alex, unaware that he has rigged devices to cleverly booby-trap his entire house. Home Alone 3 wasn't horrible, but probably shouldn't have been made, you can't just replace Macauley Culkin, Joe Pesci, or Daniel Stern. Home Alone 3 had some funny parts, but I don't like when characters are changed in a movie series, view at own risk.\"\n",
      "------------------------\n",
      "0 b\"There's a good movie lurking here, but this isn't it. The basic idea is good: to explore the moral issues that would face a group of young survivors of the apocalypse. But the logic is so muddled that it's impossible to get involved.<br /><br />For example, our four heroes are (understandably) paranoid about catching the mysterious airborne contagion that's wiped out virtually all of mankind. Yet they wear surgical masks some times, not others. Some times they're fanatical about wiping down with bleach any area touched by an infected person. Other times, they seem completely unconcerned.<br /><br />Worse, after apparently surviving some weeks or months in this new kill-or-be-killed world, these people constantly behave like total newbs. They don't bother accumulating proper equipment, or food. They're forever running out of fuel in the middle of nowhere. They don't take elementary precautions when meeting strangers. And after wading through the rotting corpses of the entire human race, they're as squeamish as sheltered debutantes. You have to constantly wonder how they could have survived this long... and even if they did, why anyone would want to make a movie about them.<br /><br />So when these dweebs stop to agonize over the moral dimensions of their actions, it's impossible to take their soul-searching seriously. Their actions would first have to make some kind of minimal sense.<br /><br />On top of all this, we must contend with the dubious acting abilities of Chris Pine. His portrayal of an arrogant young James T Kirk might have seemed shrewd, when viewed in isolation. But in Carriers he plays on exactly that same note: arrogant and boneheaded. It's impossible not to suspect that this constitutes his entire dramatic range.<br /><br />On the positive side, the film *looks* excellent. It's got an over-sharp, saturated look that really suits the southwestern US locale. But that can't save the truly feeble writing nor the paper-thin (and annoying) characters. Even if you're a fan of the end-of-the-world genre, you should save yourself the agony of watching Carriers.\"\n",
      "------------------------\n",
      "0 b'I saw this movie at an actual movie theater (probably the $2.00 one) with my cousin and uncle. We were around 11 and 12, I guess, and really into scary movies. I remember being so excited to see it because my cool uncle let us pick the movie (and we probably never got to do that again!) and sooo disappointed afterwards!! Just boring and not scary. The only redeeming thing I can remember was Corky Pigeon from Silver Spoons, and that wasn\\'t all that great, just someone I recognized. I\\'ve seen bad movies before and this one has always stuck out in my mind as the worst. This was from what I can recall, one of the most boring, non-scary, waste of our collective $6, and a waste of film. I have read some of the reviews that say it is worth a watch and I say, \"Too each his own\", but I wouldn\\'t even bother. Not even so bad it\\'s good.'\n",
      "------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-07 17:16:14.854247: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "for text_batch, label_batch in train_ds.take(1):\n",
    "  for i in range(5):\n",
    "    print(label_batch[i].numpy(), text_batch.numpy()[i])\n",
    "    print('------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c99082-e975-4063-97af-cca8a93b9d01",
   "metadata": {},
   "source": [
    "### Configure the dataset for performance\n",
    "\n",
    "These are two important methods to use when loading data to make sure that I/O does not become blocking.\n",
    "\n",
    "- `.cache()` keeps data in memory after it's loaded off disk. This will ensure the dataset does not become a bottleneck while training your model. If your dataset is too large to fit into memory, you can also use this method to create a performant on-disk cache, which is more efficient to read than many small files.\n",
    "- `prefetch()` overlaps data preprocessing and model execution while training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd29923d-b14a-420c-b651-37bc493205ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e3ed5e-908d-4c8b-a0fb-f83202071fde",
   "metadata": {},
   "source": [
    "### The Embedding Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83195202-850d-40ff-b8f7-9ea0719f75e2",
   "metadata": {},
   "source": [
    "The Embedding layer can be interpreted as a lookup table that maps from integer indices(which stand for specific words) to dense vectors(their embeddings). The dimensionality or width of the embedding is a parameter you can experiment with to see what works well for the problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d26fba6f-1dc9-4d69-9084-91e4459e74b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed a 1000 word vocabulary into 5 dimensions\n",
    "embedding_layer = tf.keras.layers.Embedding(1000, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6c4ec7-3417-4020-a695-f5877afb9ec7",
   "metadata": {},
   "source": [
    "When an Embedding layer is created, the weights for the embedding layer are randomly initialized(just like any other layer). During training, they are gradually adjusted via backpropagation. Once trained, the learned word embeddings will roughly encode similarity between words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8eeb4b-e795-4f20-95cb-b16ae1554c0d",
   "metadata": {},
   "source": [
    "If we pass an interger into the embedding layer, the result replaces each integer with the vector from the embedding table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b86c03a7-e827-4d07-b7fd-c8283eea6ff1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.0201366 , -0.00642856, -0.04087245, -0.00741005, -0.02235507],\n",
       "       [-0.04529129,  0.02532952,  0.04054601,  0.01945807, -0.04099651],\n",
       "       [ 0.01179831, -0.04293561, -0.03884371,  0.04257048, -0.00390206]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = embedding_layer(tf.constant([1, 2, 3]))\n",
    "result.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa702649-353c-47da-a7de-382aa7edcc93",
   "metadata": {},
   "source": [
    "For Text or sequence problems, the embedding layer takes a 2d tensor of integers, of shape `(samples, sequence_length)`, where each entry is a sequence of integers. It can embed sequences of variable lengths. One could feed into the embedding layer above batches with shapes(32,10)(batch of 32 sequences of length 10) or (64,15)(batch of 64 sequences of length 15). \n",
    "\n",
    "The returned tensor has one more axis than the input, the embedding vectors are aligned along the new last axis. Pass it a (2,3) input batch and the output is (2, 3, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0730ed8a-8d02-4f9f-b90d-004e00619c31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 3, 5])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = embedding_layer(tf.constant([[0,1,2],[3,4,5]]))\n",
    "result.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e425be31-17a9-447f-a825-ef3c67f8480a",
   "metadata": {},
   "source": [
    "When given a batch of sequences as input, an embedding layer returns a 3D floating point tensor, of shape (samples, sequence_length, embedding_dimensionality). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb7ed86-0d79-4b6e-babd-3a527cad69b3",
   "metadata": {},
   "source": [
    "### Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1358bb2f-a1ed-409a-928d-5d6d277b8d67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-07 17:16:27.341982: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "# Create a custom standardization function to strip HTML break tags '<br />'\n",
    "def custom_standardization(input_data):\n",
    "    lowercase = tf.strings.lower(input_data)\n",
    "\n",
    "    stripped_html = tf.strings.regex_replace(lowercase, '<br />', \" \")\n",
    "\n",
    "    return tf.strings.regex_replace(stripped_html, '[%s]' % re.escape(string.punctuation),'') # remove punctuation from the string\n",
    "\n",
    "# Vocabulary size and number of words in a sequence \n",
    "vocab_size = 10000\n",
    "sequence_length = 100 \n",
    "\n",
    "# Use the text vectorization layer to normalize, split and map strings to integers. Watch that the layer uses the custom custom_standardization defined above\n",
    "# Set maximum_sequence length as all samples are not of the same length\n",
    "vectorize_layer = TextVectorization(\n",
    "    standardize=custom_standardization, \n",
    "    max_tokens=vocab_size, \n",
    "    output_mode='int',\n",
    "    output_sequence_length=sequence_length\n",
    ")\n",
    "\n",
    "# Make a text-only dataset (no labels) and call adapt to build the vocabulary\n",
    "# NOTE - By using train_ds.map(lambda x, y: x), you effectively create a new dataset that consists only of the input features from the original dataset, discarding the labels. \n",
    "# This can be useful in scenarios where you only need to work with or analyze the features without concern for their associated labels.\n",
    "text_ds = train_ds.map(lambda x, y:x)\n",
    "vectorize_layer.adapt(text_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b138d25b-6055-495b-a901-33749605cbe7",
   "metadata": {},
   "source": [
    "### Classification Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bbe7c87d-bb62-48e0-b9de-7376d84bb76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 16\n",
    "\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "04da69b7-a96d-4306-a242-383ff05091e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(vectorize_layer)\n",
    "model.add(Embedding(vocab_size, embedding_dim, name='embedding_2'))\n",
    "model.add(GlobalAveragePooling1D())\n",
    "\n",
    "\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc881552-d2f1-4d9a-a3c7-c85f7d1d4863",
   "metadata": {},
   "source": [
    "### Compliling and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8d0084d3-0070-4dbc-9dc1-9f18ee71228d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir='logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "405937cd-eeed-41b6-ba1a-0e28f7093257",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "70c34b73-11f2-4b1e-8025-fd9c2980178f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-07 17:16:45.242201: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 101ms/step - accuracy: 0.5027 - loss: 0.6914 - val_accuracy: 0.4884 - val_loss: 0.6842\n",
      "Epoch 2/15\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - accuracy: 0.5029 - loss: 0.6810 - val_accuracy: 0.4912 - val_loss: 0.6694\n",
      "Epoch 3/15\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.5042 - loss: 0.6634 - val_accuracy: 0.4942 - val_loss: 0.6463\n",
      "Epoch 4/15\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.5174 - loss: 0.6363 - val_accuracy: 0.5552 - val_loss: 0.6151\n",
      "Epoch 5/15\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - accuracy: 0.5943 - loss: 0.6009 - val_accuracy: 0.6406 - val_loss: 0.5793\n",
      "Epoch 6/15\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - accuracy: 0.6756 - loss: 0.5609 - val_accuracy: 0.7002 - val_loss: 0.5431\n",
      "Epoch 7/15\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.7293 - loss: 0.5205 - val_accuracy: 0.7334 - val_loss: 0.5102\n",
      "Epoch 8/15\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - accuracy: 0.7641 - loss: 0.4833 - val_accuracy: 0.7532 - val_loss: 0.4825\n",
      "Epoch 9/15\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.7862 - loss: 0.4509 - val_accuracy: 0.7664 - val_loss: 0.4601\n",
      "Epoch 10/15\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - accuracy: 0.8010 - loss: 0.4235 - val_accuracy: 0.7750 - val_loss: 0.4423\n",
      "Epoch 11/15\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.8146 - loss: 0.4004 - val_accuracy: 0.7844 - val_loss: 0.4282\n",
      "Epoch 12/15\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.8253 - loss: 0.3808 - val_accuracy: 0.7898 - val_loss: 0.4170\n",
      "Epoch 13/15\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.8354 - loss: 0.3639 - val_accuracy: 0.7950 - val_loss: 0.4080\n",
      "Epoch 14/15\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.8438 - loss: 0.3493 - val_accuracy: 0.7984 - val_loss: 0.4009\n",
      "Epoch 15/15\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - accuracy: 0.8505 - loss: 0.3363 - val_accuracy: 0.8022 - val_loss: 0.3951\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1479584c0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_ds, validation_data=val_ds, epochs=15, callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5cd07d-36a3-4d55-a83a-ac1ed1c0c3d2",
   "metadata": {},
   "source": [
    "With this approach the model reaches a validation accuracy of around 80% (note that the model is overfitting since training accuracy is higher)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d9517295-34f3-46fc-aaa5-98eeb23a2eb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ text_vectorization              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TextVectorization</span>)             │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embedding_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">160,000</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling1d        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">272</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ text_vectorization              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mTextVectorization\u001b[0m)             │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embedding_2 (\u001b[38;5;33mEmbedding\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │       \u001b[38;5;34m160,000\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling1d        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │           \u001b[38;5;34m272\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m17\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">480,869</span> (1.83 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m480,869\u001b[0m (1.83 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">160,289</span> (626.13 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m160,289\u001b[0m (626.13 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">320,580</span> (1.22 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m320,580\u001b[0m (1.22 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85863b35-a2f8-4cb7-bd97-1ff43333721f",
   "metadata": {},
   "source": [
    "### Visualize the model metrics in TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "05d8ea58-e5d9-4087-a98d-8786c47e9cba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-65cbc3f23340536a\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-65cbc3f23340536a\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#docs_infra: no_execute\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d8ee80-7de2-44eb-b470-f15594b82d51",
   "metadata": {},
   "source": [
    "### Retrieve the trained word embeddings and save them to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8f88c421-1b6b-41d3-89ba-80fe26d11742",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "weights = model.get_layer('embedding_2').get_weights()[0]\n",
    "vocab = vectorize_layer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b852a70-d50f-4959-805c-68ea55f06eb6",
   "metadata": {},
   "source": [
    "Write the weights to disk. To use the Embedding Projector, you will upload two files in tab separated format: a file of vectors (containing the embedding), and a file of meta data (containing the words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e062bfbf-ee53-40a5-a471-6c7714fb210c",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_v = io.open('vectors.tsc', 'w', encoding='utf-8')\n",
    "out_m = io.open('metadata.tsc', 'w', encoding='utf-8')\n",
    "\n",
    "\n",
    "for index, word in enumerate(vocab): \n",
    "    if index == 0: \n",
    "        continue # skip 0, it`s padding\n",
    "\n",
    "    vec = weights[index]\n",
    "    out_v.write('\\t'.join([str(x) for x in vec]) + '\\n')\n",
    "    out_m.write(word + '\\n') \n",
    "out_v.close()\n",
    "out_m.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "word-embeddings",
   "language": "python",
   "name": "word-embeddings"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
